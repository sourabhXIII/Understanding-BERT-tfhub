{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_polarity.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoYi2HYm8dHV",
        "colab_type": "text"
      },
      "source": [
        "Purpose of this notebook is to see how to prepare the data to use the tfhub BERT module. I tried to explain the data structures it uses and exactly how you can prepare your data set in a tfhub BERT consumable format. For this I used ```bert-tensorflow``` package hosted [here](https://github.com/google-research/bert).\n",
        "\n",
        "This tutorial [page](https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb) from Google shows how to do this but uses tf.estimator. I am not very comfortable with tf.estimators and like to build models with custom layers and tf.keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qUAOv3qLh58",
        "colab_type": "text"
      },
      "source": [
        "#### Start by importing some libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1awLhEvcOw1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# example followed: \n",
        "# https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb\n",
        "# https://github.com/SunYanCN/bert-text\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import re\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2iOkTdMLtna",
        "colab_type": "text"
      },
      "source": [
        "#### We need google provided bert libraries.\n",
        "This will make our life easy to *tokenize*, *featurise* our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Knl1eszRMDAl",
        "colab_type": "code",
        "outputId": "d11b8514-3f81-4f0c-96f1-11f8571eb866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYDG-MxWO6f5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import bert related packages\n",
        "import bert\n",
        "from bert import modeling\n",
        "from bert import run_classifier\n",
        "from bert import tokenization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGwv9OF6MStC",
        "colab_type": "text"
      },
      "source": [
        "### Dataset\n",
        "This is a small dataset to validate our idea."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSjhLmOsPLX1",
        "colab_type": "code",
        "outputId": "68095744-7a8c-47cc-f58e-7d70b08cddb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# prepare dataset\n",
        "data = ['he is happy because he got a new car'\n",
        "        ,'He is a lovable person'\n",
        "        ,'john is a cheerful guy'\n",
        "        ,'he was in a merry mood'\n",
        "        ,'the whole crowd was joyful'\n",
        "        ,'she is a loving person'\n",
        "        ,'he was delighted to see me'\n",
        "        ,'he was smiling at me when i got a new mobile'\n",
        "        ,'he is in a jovial mood'\n",
        "        ,'he was sad because his friend died'\n",
        "        ,'he was unhappy with his dog'\n",
        "        ,'the company has miserable status'\n",
        "        ,'he was sorrowful as he lost all his money'\n",
        "        ,'The bird was sorrowful as it had no food'\n",
        "        ,'the dog was glum as his master was not at home'\n",
        "        ,'he was in gloomy mood'\n",
        "        ,'she was depressed as she lost her job'\n",
        "        ,'do not be downhearted']\n",
        "label = [1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0]\n",
        "\n",
        "zip_list = list(zip(data,label))\n",
        "df = pd.DataFrame(zip_list, columns = ['sentence','polarity'])\n",
        "train_df, test_df = train_test_split(df, test_size=0.1)\n",
        "\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "test_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "DATA_COLUMN = 'sentence'\n",
        "LABEL_COLUMN = 'polarity'\n",
        "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
        "label_list = [0, 1]\n",
        "\n",
        "\n",
        "print('Train data shape:', train_df.shape)\n",
        "print('Test data shape:', test_df.shape)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data shape: (16, 2)\n",
            "Test data shape: (2, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TnB7B_S-K01",
        "colab_type": "text"
      },
      "source": [
        "### Prepare dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69x1EmNr-dfp",
        "colab_type": "text"
      },
      "source": [
        "#### **InputExample**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtNWIhqLNLAO",
        "colab_type": "text"
      },
      "source": [
        "To use with BERT we need to give our data a special shape.<br></br>\n",
        "**Details about the ```InputExample```.**<br></br>\n",
        "\n",
        "See the definition of ```InputExample```. Taken from BERT source code [here](https://github.com/google-research/bert/blob/master/run_classifier.py).\n",
        "```\n",
        "class InputExample(object):\n",
        "  \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "  def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "    \"\"\"Constructs a InputExample.\n",
        "    Args:\n",
        "      guid: Unique id for the example.\n",
        "      text_a: string. The untokenized text of the first sequence. For single\n",
        "        sequence tasks, only this sequence must be specified.\n",
        "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "        Only must be specified for sequence pair tasks.\n",
        "      label: (Optional) string. The label of the example. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "    self.guid = guid\n",
        "    self.text_a = text_a\n",
        "    self.text_b = text_b\n",
        "    self.label = label\n",
        "```\n",
        "\n",
        "Details of the memebers:\n",
        "- ```text_a``` is the text we want to classify, in our case, it is the ```sentence``` column of the Dataframe.\n",
        "- ```text_b``` is used if we're training a model to understand the relationship between sentences ```text_a``` and ```text_b``` (i.e. is ```text_b``` a translation of ```text_a```? Can ```text_b``` be the next sentence to ```text_a```?). This doesn't apply to our task, so we can leave ```text_b``` as ```None```.\n",
        "- ```label``` is the class label according to our example.\n",
        "\n",
        "So, one can assume ```InputExample``` as a data structure to hold sentence pairs (if a pair is required) and the target label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR4HjlE_Qjzj",
        "colab_type": "code",
        "outputId": "689d1ba7-1d8a-445b-8788-75f5f1e0e190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
        "train_InputExamples = train_df.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "test_InputExamples = test_df.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "print(train_InputExamples[0].text_a)\n",
        "print(train_InputExamples[0].label)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "he was smiling at me when i got a new mobile\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TG46X78-jPs",
        "colab_type": "text"
      },
      "source": [
        "#### Tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zkq76Hm7QZWT",
        "colab_type": "text"
      },
      "source": [
        "**Now we must tokenize our data using the same tokenizer used to train the actual BERT model.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Juiz46TlR6jc",
        "colab_type": "code",
        "outputId": "729ee580-3b7b-48a4-8842-753e675c748a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# preprocess our data so that it matches the data BERT was trained on\n",
        "\n",
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "  return bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()\n",
        "\n",
        "# see what tokenizer does\n",
        "print(tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\"))\n",
        "print(tokenizer.tokenize(train_df['sentence'][0]))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['this', 'here', \"'\", 's', 'an', 'example', 'of', 'using', 'the', 'bert', 'token', '##izer']\n",
            "['he', 'was', 'smiling', 'at', 'me', 'when', 'i', 'got', 'a', 'new', 'mobile']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Dq12Zjg-7x8",
        "colab_type": "text"
      },
      "source": [
        "#### InputFeatures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK2guJnQQNf6",
        "colab_type": "text"
      },
      "source": [
        "But its not over yet! We are yet to get our data to a form which can be understood by BERT.\n",
        "See the below snippet. Taken from the BERT source code [here](https://github.com/google-research/bert/blob/master/run_classifier.py). We have to bring our data into the below form, ```bert-tensorflow``` provides us the necessary method to do so!\n",
        "But one good thing is, the next step is going to be the last step to transform our data into BERT consumable form.\n",
        "\n",
        "```\n",
        "# The convention in BERT is:\n",
        "# (a) For sequence pairs:\n",
        "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "#  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
        "# (b) For single sequences:\n",
        "#  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "#  type_ids: 0     0   0   0  0     0 0\n",
        "#\n",
        "# Where \"type_ids\" are used to indicate whether this is the first\n",
        "# sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "# `type=1` were learned during pre-training and are added to the wordpiece\n",
        "# embedding vector (and position vector). This is not *strictly* necessary\n",
        "# since the [SEP] token unambiguously separates the sequences, but it makes\n",
        "# it easier for the model to learn the concept of sequences.\n",
        "#\n",
        "# For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "# used as the \"sentence vector\". Note that this only makes sense because\n",
        "# the entire model is fine-tuned.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZUvfkKeS5gT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using our tokenizer, we'll call run_classifier.convert_examples_to_features on our InputExamples to convert them into features BERT understands\n",
        "MAX_SEQ_LENGTH = 20 # at most these many tokens long\n",
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_hrGMIAX9qj",
        "colab_type": "text"
      },
      "source": [
        "So, what are ```InputFeatures``` now? <br>\n",
        "It is again a data structure to hold three pieces of information.\n",
        "- ```input_ids```: Index of the tokens in the vocabulary.\n",
        "- ```input_mask```: The mask value is 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
        "- ```segment_ids```: It is the ```type_ids```. It is 0 for the tokens of ```text_a``` and 1 for the tokens of ```text_b```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWheGEpcZUM6",
        "colab_type": "code",
        "outputId": "d594f2f1-1f17-4dd6-f659-ec4c16fbca8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Lets understand the InputFeatures data structures by printing values for a single data row.\n",
        "print('Text:', train_InputExamples[0].text_a)\n",
        "print('Tokens:', tokenizer.tokenize(train_InputExamples[0].text_a))\n",
        "print('Vocab index (input_ids):', train_features[0].input_ids)\n",
        "print('Get back the tokens from vocab index:', tokenizer.convert_ids_to_tokens(train_features[0].input_ids))\n",
        "print('input_mask:', train_features[0].input_mask)\n",
        "print('segment_ids:', train_features[0].segment_ids)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: he was smiling at me when i got a new mobile\n",
            "Tokens: ['he', 'was', 'smiling', 'at', 'me', 'when', 'i', 'got', 'a', 'new', 'mobile']\n",
            "Vocab index (input_ids): [101, 2002, 2001, 5629, 2012, 2033, 2043, 1045, 2288, 1037, 2047, 4684, 102, 0, 0, 0, 0, 0, 0, 0]\n",
            "Get back the tokens from vocab index: ['[CLS]', 'he', 'was', 'smiling', 'at', 'me', 'when', 'i', 'got', 'a', 'new', 'mobile', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEb4HzIy_AaK",
        "colab_type": "text"
      },
      "source": [
        "### Model building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8sYvmbl5EjL",
        "colab_type": "text"
      },
      "source": [
        "We are all set to start with the model!\n",
        "As a first step we will create a custom layer by wrapping the tensorflow hub BERT model.<br>\n",
        "Once done, we can use the BERT layer as any other layer inside a keras model. This way we will create an abstraction for the actual BERT model from tensorflow hub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfBga8FM_HrS",
        "colab_type": "text"
      },
      "source": [
        "#### Custom layer to wrap tfhub BERT module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tikzjCC7TzTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "class BertLayer(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_fine_tune_layers=10,\n",
        "        pooling=\"first\",\n",
        "        bert_path=None,\n",
        "        max_len = None,\n",
        "        return_sequences=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.n_fine_tune_layers = n_fine_tune_layers\n",
        "        self.trainable = True\n",
        "        self.output_size = 768\n",
        "        self.pooling = pooling\n",
        "        self.bert_path = bert_path\n",
        "        self.return_sequences = return_sequences\n",
        "        self.output_key = 'sequence_output' if return_sequences else 'pooled_output'\n",
        "        self.max_len = max_len\n",
        "        if self.pooling not in [\"first\", \"mean\"]:\n",
        "            raise NameError(\n",
        "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
        "            )\n",
        "\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.trainable = self.n_fine_tune_layers > 0\n",
        "        self.bert = hub.Module(\n",
        "            self.bert_path, trainable=self.trainable, name=f\"{self.name}_module\"\n",
        "        )\n",
        "        # Remove unused layers\n",
        "        trainable_vars = self.bert.variables\n",
        "\n",
        "        if self.pooling == \"first\":\n",
        "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
        "            trainable_layers = [\"pooler/dense\"]\n",
        "\n",
        "        elif self.pooling == \"mean\":\n",
        "            trainable_vars = [\n",
        "                var\n",
        "                for var in trainable_vars\n",
        "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
        "            ]\n",
        "            trainable_layers = []\n",
        "        else:\n",
        "            raise NameError(\n",
        "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
        "            )\n",
        "\n",
        "        # Select how many layers to fine tune\n",
        "        for i in range(self.n_fine_tune_layers):\n",
        "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
        "\n",
        "        # Update trainable vars to contain only the specified layers\n",
        "        trainable_vars = [\n",
        "            var\n",
        "            for var in trainable_vars\n",
        "            if any([l in var.name for l in trainable_layers])\n",
        "        ]\n",
        "\n",
        "        # Add to trainable weights\n",
        "        for var in trainable_vars:\n",
        "            self._trainable_weights.append(var)\n",
        "\n",
        "        for var in self.bert.variables:\n",
        "            if var not in self._trainable_weights:\n",
        "                self._non_trainable_weights.append(var)\n",
        "\n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
        "        input_ids, input_mask, segment_ids = inputs\n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "\n",
        "        if self.pooling == \"first\":\n",
        "            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
        "                \"pooled_output\"\n",
        "            ]\n",
        "        elif self.pooling == \"mean\":\n",
        "            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
        "                \"sequence_output\"\n",
        "            ]\n",
        "\n",
        "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
        "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
        "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
        "            input_mask = tf.cast(input_mask, tf.float32)\n",
        "            pooled = masked_reduce_mean(result, input_mask)\n",
        "        else:\n",
        "            raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n",
        "\n",
        "        sequence_output = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\"sequence_output\"]\n",
        "        sequence_output = tf.reshape(sequence_output, (-1, self.max_len, self.output_size))\n",
        "        return [sequence_output, pooled]\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return [(input_shape[0], self.max_len, self.output_size),(input_shape[0], self.output_size)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Agj5UUx_Ptr",
        "colab_type": "text"
      },
      "source": [
        "#### Keras Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJS84hcA5tYo",
        "colab_type": "text"
      },
      "source": [
        "Build the model which uses BERT as a internal layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-_ME63sT1Nh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(bert_path, max_seq_length):\n",
        "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
        "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
        "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
        "    bert_inputs = [in_id, in_mask, in_segment]\n",
        "\n",
        "    bert_output = BertLayer(bert_path=bert_path, pooling='first', n_fine_tune_layers=0, max_len=MAX_SEQ_LENGTH)(bert_inputs)\n",
        "\n",
        "    dense = tf.keras.layers.Dense(128, activation=\"relu\")(bert_output[1])\n",
        "    pred = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dense)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "  \n",
        "def initialize_vars(allow_growth=True):\n",
        "    gpu_options = tf.GPUOptions(allow_growth=allow_growth)\n",
        "    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    K.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeYjPMGJ_aaB",
        "colab_type": "text"
      },
      "source": [
        "#### Unpack InputFeatures to feed the training data into the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJTJrI0e7OjX",
        "colab_type": "text"
      },
      "source": [
        "Unpack the ```InputFeatures``` to get the inputs to our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNOd7Cn_YJLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# our model needs the following inputs\n",
        "train_input_ids = []\n",
        "train_input_mask = []\n",
        "train_segment_ids = []\n",
        "train_label_ids = []\n",
        "for feature in train_features:\n",
        "    train_input_ids.append(feature.input_ids)\n",
        "    train_input_mask.append(feature.input_mask)\n",
        "    train_segment_ids.append(feature.segment_ids)\n",
        "    train_label_ids.append(feature.label_id)\n",
        "\n",
        "test_input_ids = []\n",
        "test_input_mask = []\n",
        "test_segment_ids = []\n",
        "test_label_ids = []\n",
        "for feature in test_features:\n",
        "    test_input_ids.append(feature.input_ids)\n",
        "    test_input_mask.append(feature.input_mask)\n",
        "    test_segment_ids.append(feature.segment_ids)\n",
        "    test_label_ids.append(feature.label_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P1QWZmD_XOz",
        "colab_type": "text"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWFUIwX07dBC",
        "colab_type": "text"
      },
      "source": [
        "Now, train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XB5QrjeYWxo",
        "colab_type": "code",
        "outputId": "f751b8b3-8d4f-4edd-e9e0-64e24500a574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "model = build_model(BERT_MODEL_HUB, MAX_SEQ_LENGTH)\n",
        "\n",
        "initialize_vars()\n",
        "\n",
        "train_inputs = [train_input_ids, train_input_mask, train_segment_ids]\n",
        "train_labels = train_label_ids\n",
        "\n",
        "model.fit(train_inputs, train_labels,\n",
        "          validation_data=None,\n",
        "          epochs=10,batch_size=2,shuffle=True )"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert_layer_7 (BertLayer)        [(None, 20, 768), (N 110104890   input_ids[0][0]                  \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 128)          98432       bert_layer_7[0][1]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 1)            129         dense_14[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 110,203,451\n",
            "Trainable params: 98,561\n",
            "Non-trainable params: 110,104,890\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "16/16 [==============================] - 4s 272ms/sample - loss: 1.0198 - acc: 0.5625\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - 2s 135ms/sample - loss: 0.7072 - acc: 0.5625\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - 2s 132ms/sample - loss: 0.7507 - acc: 0.6875\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - 2s 126ms/sample - loss: 0.7840 - acc: 0.6250\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - 2s 119ms/sample - loss: 0.8233 - acc: 0.3750\n",
            "Epoch 6/10\n",
            "16/16 [==============================] - 2s 120ms/sample - loss: 0.5735 - acc: 0.5625\n",
            "Epoch 7/10\n",
            "16/16 [==============================] - 2s 120ms/sample - loss: 0.6617 - acc: 0.6250\n",
            "Epoch 8/10\n",
            "16/16 [==============================] - 2s 119ms/sample - loss: 0.5750 - acc: 0.8125\n",
            "Epoch 9/10\n",
            "16/16 [==============================] - 2s 120ms/sample - loss: 0.5532 - acc: 0.8125\n",
            "Epoch 10/10\n",
            "16/16 [==============================] - 2s 121ms/sample - loss: 0.5732 - acc: 0.7500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9c7e9735c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG8RoSA5_jTQ",
        "colab_type": "text"
      },
      "source": [
        "#### Get prediction with the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GThbNdbJ3uwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getPredictionFromSentence(in_sentences):\n",
        "  labels = [\"Negative\", \"Positive\"]\n",
        "  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
        "  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "  test_input_ids = []\n",
        "  test_input_mask = []\n",
        "  test_segment_ids = []\n",
        "  test_label_ids = []\n",
        "  for feature in input_features:\n",
        "      test_input_ids.append(feature.input_ids)\n",
        "      test_input_mask.append(feature.input_mask)\n",
        "      test_segment_ids.append(feature.segment_ids)\n",
        "      test_label_ids.append(feature.label_id)\n",
        "  \n",
        "  probabilities = getPredictionFromFeatures(test_input_ids, test_input_mask, test_segment_ids)\n",
        "  # print(probabilities)\n",
        "  predictions = (probabilities > 0.5).astype(np.int)\n",
        "  # print(predictions)\n",
        "  \n",
        "  return [(sentence, proba, labels[prediction[0]]) for sentence, proba, prediction in zip(in_sentences, probabilities, predictions)]\n",
        "\n",
        "def getPredictionFromFeatures(test_input_ids, test_input_mask, test_segment_ids):\n",
        "  return model.predict([test_input_ids, test_input_mask, test_segment_ids])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOyy669X_oo3",
        "colab_type": "text"
      },
      "source": [
        "#### Test data and prediction on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSOAiujIDweR",
        "colab_type": "code",
        "outputId": "47039689-66ab-4c8b-e129-e6ace921e588",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# get little more test data\n",
        "pred_sentences = [\n",
        "  \"That movie was absolutely awful\",\n",
        "  \"The acting was a bit lacking\",\n",
        "  \"The film was creative and surprising\",\n",
        "  \"Absolutely fantastic!\"\n",
        "]\n",
        "\n",
        "predictions = getPredictionFromSentence(pred_sentences)\n",
        "print(predictions)\n",
        "\n",
        "# test with our original test data\n",
        "predictions = getPredictionFromSentence(test_df['sentence'].tolist())\n",
        "print(predictions)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('That movie was absolutely awful', array([0.7265612], dtype=float32), 'Positive'), ('The acting was a bit lacking', array([0.5497797], dtype=float32), 'Positive'), ('The film was creative and surprising', array([0.5154184], dtype=float32), 'Positive'), ('Absolutely fantastic!', array([0.561795], dtype=float32), 'Positive')]\n",
            "[('he was sorrowful as he lost all his money', array([0.699868], dtype=float32), 'Positive'), ('she was depressed as she lost her job', array([0.5356732], dtype=float32), 'Positive')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYgSIjrB_sUl",
        "colab_type": "text"
      },
      "source": [
        "### Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak640w2SwpUP",
        "colab_type": "text"
      },
      "source": [
        "Now, suppose you want to pull out the activations of some of the layers and use them as features for your downstream work. But which layers provide the best results? Below image from [Jay Alammar](http://jalammar.github.io/illustrated-bert/)'s blog summarizes the findings of the paper.\n",
        "![Bert-feature_extraction](http://jalammar.github.io/images/bert-feature-extraction-contextualized-embeddings.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0gbE6dZ_xpc",
        "colab_type": "text"
      },
      "source": [
        "#### Extract features from a specific layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BItX31YyIAY",
        "colab_type": "text"
      },
      "source": [
        "Here, we will extract the activation from some random layer, but you can follow that to extract features from any layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pFlk0TAWrRC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "36312003-fa09-4726-db65-6442cbcdbf15"
      },
      "source": [
        "# collect all the layers of the tfhub bert model\n",
        "# layers_of_bert = [i.values() for i in tf.get_default_graph().get_operations()]\n",
        "# check their names\n",
        "# print(layers_of_bert[-15:]) # there are just too many layersß\n",
        "\n",
        "# picked some random layer\n",
        "some_layer_name = 'bert_layer_module/bert/encoder/layer_3/attention/output/LayerNorm/moments/SquaredDifference:0'\n",
        "\n",
        "# this helped: https://stackoverflow.com/questions/55333558/how-to-access-bert-intermediate-layer-outputs-in-tf-hub-module\n",
        "some_layer_output = K.get_session().run(tf.get_default_graph().get_tensor_by_name(some_layer_name)\n",
        "      , feed_dict={'bert_layer_module/input_ids:0': test_input_ids, 'bert_layer_module/input_mask:0': test_input_mask, 'bert_layer_module/segment_ids:0': test_segment_ids})\n",
        "print('Input data shape:', len(test_input_ids))\n",
        "print('Feature shape: {}, remember MAX_SEQ_LENGTH value is: {}'.format(some_layer_output.shape, MAX_SEQ_LENGTH))\n",
        "print('Feature matrix:')\n",
        "print(some_layer_output)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data shape: 2\n",
            "Feature shape: (40, 768), remember MAX_SEQ_LENGTH value is: 20\n",
            "Feature matrix:\n",
            "[[0.02290059 0.03542009 0.08184339 ... 0.03956602 0.0046551  0.09658112]\n",
            " [0.00689361 0.00321472 0.6773902  ... 0.00224064 0.00283264 0.03928791]\n",
            " [0.00213189 0.32839078 0.1467535  ... 0.00231929 0.0064002  0.08923845]\n",
            " ...\n",
            " [0.08666013 0.13227108 0.27587062 ... 0.797492   0.18822151 0.42659256]\n",
            " [0.15041502 0.18689519 0.23036228 ... 0.8800924  0.1415598  0.34996518]\n",
            " [0.27268362 0.23202768 0.06867391 ... 0.9788965  0.1378572  0.24066505]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj1SymRg_49_",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEl7STjwAEgv",
        "colab_type": "text"
      },
      "source": [
        "There might be better way to do this. I will update if I find a more elegant way of doing the same."
      ]
    }
  ]
}