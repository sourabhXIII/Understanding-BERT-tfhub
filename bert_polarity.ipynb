{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_polarity.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qUAOv3qLh58",
        "colab_type": "text"
      },
      "source": [
        "#Start by importing some libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1awLhEvcOw1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# example followed: \n",
        "# https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb\n",
        "# https://github.com/SunYanCN/bert-text\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import re\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2iOkTdMLtna",
        "colab_type": "text"
      },
      "source": [
        "### We need google provided bert libraries.\n",
        "This will make our life easy to *tokenize*, *featurise* our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Knl1eszRMDAl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a9ed0dc3-bbf3-4962-d911-c948e1887e1e"
      },
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYDG-MxWO6f5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "89e406f1-e2c8-4f11-fe01-935782a5cdca"
      },
      "source": [
        "# import bert related packages\n",
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import tokenization"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0830 19:04:43.044165 139774592931712 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGwv9OF6MStC",
        "colab_type": "text"
      },
      "source": [
        "### Dataset\n",
        "This is a small dataset to validate our idea.a"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSjhLmOsPLX1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f7411025-7330-4273-eeed-3fed0f9e1100"
      },
      "source": [
        "# prepare dataset\n",
        "data = ['he is happy because he got a new car'\n",
        "        ,'He is a lovable person'\n",
        "        ,'john is a cheerful guy'\n",
        "        ,'he was in a merry mood'\n",
        "        ,'the whole crowd was joyful'\n",
        "        ,'she is a loving person'\n",
        "        ,'he was delighted to see me'\n",
        "        ,'he was smiling at me when i got a new mobile'\n",
        "        ,'he is in a jovial mood'\n",
        "        ,'he was sad because his friend died'\n",
        "        ,'he was unhappy with his dog'\n",
        "        ,'the company has miserable status'\n",
        "        ,'he was sorrowful as he lost all his money'\n",
        "        ,'The bird was sorrowful as it had no food'\n",
        "        ,'the dog was glum as his master was not at home'\n",
        "        ,'he was in gloomy mood'\n",
        "        ,'she was depressed as she lost her job'\n",
        "        ,'do not be downhearted']\n",
        "label = [1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0]\n",
        "\n",
        "zip_list = list(zip(data,label))\n",
        "df = pd.DataFrame(zip_list, columns = ['sentence','polarity'])\n",
        "train_df, test_df = train_test_split(df, test_size=0.1)\n",
        "\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "test_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "DATA_COLUMN = 'sentence'\n",
        "LABEL_COLUMN = 'polarity'\n",
        "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
        "label_list = [0, 1]\n",
        "\n",
        "\n",
        "print('Train data shape:', train_df.shape)\n",
        "print('Test data shape:', test_df.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data shape: (16, 2)\n",
            "Test data shape: (2, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtNWIhqLNLAO",
        "colab_type": "text"
      },
      "source": [
        "To use with BERT we need to give our data a special shape.<br></br>\n",
        "**Details about the ```InputExample```.**<br></br>\n",
        "\n",
        "See the definition of ```InputExample```. Taken from BERT source code [here](https://github.com/google-research/bert/blob/master/run_classifier.py).\n",
        "```\n",
        "class InputExample(object):\n",
        "  \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "  def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "    \"\"\"Constructs a InputExample.\n",
        "    Args:\n",
        "      guid: Unique id for the example.\n",
        "      text_a: string. The untokenized text of the first sequence. For single\n",
        "        sequence tasks, only this sequence must be specified.\n",
        "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "        Only must be specified for sequence pair tasks.\n",
        "      label: (Optional) string. The label of the example. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "    self.guid = guid\n",
        "    self.text_a = text_a\n",
        "    self.text_b = text_b\n",
        "    self.label = label\n",
        "```\n",
        "\n",
        "Details of the memebers:\n",
        "- ```text_a``` is the text we want to classify, in our case, it is the ```sentence``` column of the Dataframe.\n",
        "- ```text_b``` is used if we're training a model to understand the relationship between sentences ```text_a``` and ```text_b``` (i.e. is ```text_b``` a translation of ```text_a```? Can ```text_b``` be the next sentence to ```text_a```?). This doesn't apply to our task, so we can leave ```text_b``` as ```None```.\n",
        "- ```label``` is the class label according to our example.\n",
        "\n",
        "So, one can assume ```InputExample``` as a data structure to hold sentence pairs (if a pair is required) and the target label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR4HjlE_Qjzj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ed607a68-4489-46ea-89b7-43a2e3f3a759"
      },
      "source": [
        "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
        "train_InputExamples = train_df.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "test_InputExamples = test_df.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "print(train_InputExamples[0].text_a)\n",
        "print(train_InputExamples[0].label)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "he was unhappy with his dog\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zkq76Hm7QZWT",
        "colab_type": "text"
      },
      "source": [
        "Now we must tokenize our data using the same tokenizer used to train the actual BERT model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Juiz46TlR6jc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "cc31398c-f596-4822-a0c1-51f391244cfd"
      },
      "source": [
        "# preprocess our data so that it matches the data BERT was trained on\n",
        "\n",
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "  return bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()\n",
        "\n",
        "# see what tokenizer does\n",
        "print(tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\"))\n",
        "print(tokenizer.tokenize(train_df['sentence'][0]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0830 19:04:52.085046 139774592931712 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['this', 'here', \"'\", 's', 'an', 'example', 'of', 'using', 'the', 'bert', 'token', '##izer']\n",
            "['he', 'was', 'unhappy', 'with', 'his', 'dog']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK2guJnQQNf6",
        "colab_type": "text"
      },
      "source": [
        "But its not over yet! We are yet to get our data to a form which can be understood by BERT.\n",
        "See the below snippet. Taken from the BERT source code [here](https://github.com/google-research/bert/blob/master/run_classifier.py). We have to bring our data into the below form, ```bert-tensorflow``` provides us the necessary method to do so!\n",
        "But one good thing is, the next step is going to be the last step to transform our data into BERT consumable form.\n",
        "\n",
        "```\n",
        "# The convention in BERT is:\n",
        "# (a) For sequence pairs:\n",
        "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "#  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
        "# (b) For single sequences:\n",
        "#  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "#  type_ids: 0     0   0   0  0     0 0\n",
        "#\n",
        "# Where \"type_ids\" are used to indicate whether this is the first\n",
        "# sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "# `type=1` were learned during pre-training and are added to the wordpiece\n",
        "# embedding vector (and position vector). This is not *strictly* necessary\n",
        "# since the [SEP] token unambiguously separates the sequences, but it makes\n",
        "# it easier for the model to learn the concept of sequences.\n",
        "#\n",
        "# For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "# used as the \"sentence vector\". Note that this only makes sense because\n",
        "# the entire model is fine-tuned.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZUvfkKeS5gT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "bc9f69af-882c-408e-8fba-0f631144e972"
      },
      "source": [
        "# Using our tokenizer, we'll call run_classifier.convert_examples_to_features on our InputExamples to convert them into features BERT understands\n",
        "MAX_SEQ_LENGTH = 20 # at most these many tokens long\n",
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0830 19:04:52.209978 139774592931712 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_hrGMIAX9qj",
        "colab_type": "text"
      },
      "source": [
        "So, what are ```InputFeatures``` now? <br>\n",
        "It is again a data structure to hold three pieces of information.\n",
        "- ```input_ids```: Index of the tokens in the vocabulary.\n",
        "- ```input_mask```: The mask value is 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
        "- ```segment_ids```: It is the ```type_ids```. It is 0 for the tokens of ```text_a``` and 1 for the tokens of ```text_b```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWheGEpcZUM6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "b7223aeb-f46b-48ba-9d79-64708871b4d6"
      },
      "source": [
        "# Lets understand the InputFeatures data structures by printing values for a single data row.\n",
        "print('Text:', train_InputExamples[0].text_a)\n",
        "print('Tokens:', tokenizer.tokenize(train_InputExamples[0].text_a))\n",
        "print('Vocab index (input_ids):', train_features[0].input_ids)\n",
        "print('Get back the tokens from vocab index:', tokenizer.convert_ids_to_tokens(train_features[0].input_ids))\n",
        "print('input_mask:', train_features[0].input_mask)\n",
        "print('segment_ids:', train_features[0].segment_ids)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: he was unhappy with his dog\n",
            "Tokens: ['he', 'was', 'unhappy', 'with', 'his', 'dog']\n",
            "Vocab index (input_ids): [101, 2002, 2001, 12511, 2007, 2010, 3899, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Get back the tokens from vocab index: ['[CLS]', 'he', 'was', 'unhappy', 'with', 'his', 'dog', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8sYvmbl5EjL",
        "colab_type": "text"
      },
      "source": [
        "We are all set to start with the model!\n",
        "As a first step we will create a custom layer by wrapping the tensorflow hub BERT model.<br>\n",
        "Once done, we can use the BERT layer as any other layer inside a keras model. This way we will create an abstraction for the actual BERT model from tensorflow hub."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tikzjCC7TzTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "class BertLayer(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_fine_tune_layers=10,\n",
        "        pooling=\"first\",\n",
        "        bert_path=None,\n",
        "        max_len = None,\n",
        "        return_sequences=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.n_fine_tune_layers = n_fine_tune_layers\n",
        "        self.trainable = True\n",
        "        self.output_size = 768\n",
        "        self.pooling = pooling\n",
        "        self.bert_path = bert_path\n",
        "        self.return_sequences = return_sequences\n",
        "        self.output_key = 'sequence_output' if return_sequences else 'pooled_output'\n",
        "        self.max_len = max_len\n",
        "        if self.pooling not in [\"first\", \"mean\"]:\n",
        "            raise NameError(\n",
        "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
        "            )\n",
        "\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.trainable = self.n_fine_tune_layers > 0\n",
        "        self.bert = hub.Module(\n",
        "            self.bert_path, trainable=self.trainable, name=f\"{self.name}_module\"\n",
        "        )\n",
        "        # Remove unused layers\n",
        "        trainable_vars = self.bert.variables\n",
        "\n",
        "        if self.pooling == \"first\":\n",
        "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
        "            trainable_layers = [\"pooler/dense\"]\n",
        "\n",
        "        elif self.pooling == \"mean\":\n",
        "            trainable_vars = [\n",
        "                var\n",
        "                for var in trainable_vars\n",
        "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
        "            ]\n",
        "            trainable_layers = []\n",
        "        else:\n",
        "            raise NameError(\n",
        "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
        "            )\n",
        "\n",
        "        # Select how many layers to fine tune\n",
        "        for i in range(self.n_fine_tune_layers):\n",
        "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
        "\n",
        "        # Update trainable vars to contain only the specified layers\n",
        "        trainable_vars = [\n",
        "            var\n",
        "            for var in trainable_vars\n",
        "            if any([l in var.name for l in trainable_layers])\n",
        "        ]\n",
        "\n",
        "        # Add to trainable weights\n",
        "        for var in trainable_vars:\n",
        "            self._trainable_weights.append(var)\n",
        "\n",
        "        for var in self.bert.variables:\n",
        "            if var not in self._trainable_weights:\n",
        "                self._non_trainable_weights.append(var)\n",
        "\n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
        "        input_ids, input_mask, segment_ids = inputs\n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "\n",
        "        if self.pooling == \"first\":\n",
        "            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
        "                \"pooled_output\"\n",
        "            ]\n",
        "        elif self.pooling == \"mean\":\n",
        "            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
        "                \"sequence_output\"\n",
        "            ]\n",
        "\n",
        "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
        "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
        "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
        "            input_mask = tf.cast(input_mask, tf.float32)\n",
        "            pooled = masked_reduce_mean(result, input_mask)\n",
        "        else:\n",
        "            raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n",
        "\n",
        "        sequence_output = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\"sequence_output\"]\n",
        "        sequence_output = tf.reshape(sequence_output, (-1, self.max_len, self.output_size))\n",
        "        return [sequence_output, pooled]\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return [(input_shape[0], self.max_len, self.output_size),(input_shape[0], self.output_size)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJS84hcA5tYo",
        "colab_type": "text"
      },
      "source": [
        "Build the model which uses BERT as a internal layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-_ME63sT1Nh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(bert_path, max_seq_length):\n",
        "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
        "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
        "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
        "    bert_inputs = [in_id, in_mask, in_segment]\n",
        "\n",
        "    bert_output = BertLayer(bert_path=bert_path, pooling='first', n_fine_tune_layers=0, max_len=MAX_SEQ_LENGTH)(bert_inputs)\n",
        "    dense = tf.keras.layers.Dense(128, activation=\"relu\")(bert_output[1])\n",
        "    pred = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dense)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "  \n",
        "def initialize_vars(allow_growth=True):\n",
        "    gpu_options = tf.GPUOptions(allow_growth=allow_growth)\n",
        "    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    K.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJTJrI0e7OjX",
        "colab_type": "text"
      },
      "source": [
        "Unpack the ```InputFeatures``` to get the inputs to our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNOd7Cn_YJLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# our model needs the following inputs\n",
        "train_input_ids = []\n",
        "train_input_mask = []\n",
        "train_segment_ids = []\n",
        "train_label_ids = []\n",
        "for feature in train_features:\n",
        "    train_input_ids.append(feature.input_ids)\n",
        "    train_input_mask.append(feature.input_mask)\n",
        "    train_segment_ids.append(feature.segment_ids)\n",
        "    train_label_ids.append(feature.label_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWFUIwX07dBC",
        "colab_type": "text"
      },
      "source": [
        "Now, train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XB5QrjeYWxo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "outputId": "b4388654-8e24-4724-f27e-a6cad78581c5"
      },
      "source": [
        "model = build_model(BERT_MODEL_HUB, MAX_SEQ_LENGTH)\n",
        "\n",
        "initialize_vars()\n",
        "\n",
        "train_inputs = [train_input_ids, train_input_mask, train_segment_ids]\n",
        "train_labels = train_label_ids\n",
        "\n",
        "model.fit(train_inputs, train_labels,\n",
        "          validation_data=None,\n",
        "          epochs=10,batch_size=2,shuffle=True )"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0830 19:05:05.071836 139774592931712 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0830 19:05:05.142575 139774592931712 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert_layer (BertLayer)          [(None, 20, 768), (N 110104890   input_ids[0][0]                  \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          98432       bert_layer[0][1]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            129         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 110,203,451\n",
            "Trainable params: 98,561\n",
            "Non-trainable params: 110,104,890\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "16/16 [==============================] - 4s 232ms/sample - loss: 0.8721 - acc: 0.3125\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - 2s 115ms/sample - loss: 0.6535 - acc: 0.6250\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - 2s 128ms/sample - loss: 0.6594 - acc: 0.6250\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - 2s 113ms/sample - loss: 0.6063 - acc: 0.6875\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - 2s 118ms/sample - loss: 0.5091 - acc: 0.8125\n",
            "Epoch 6/10\n",
            "16/16 [==============================] - 2s 117ms/sample - loss: 0.5297 - acc: 0.6250\n",
            "Epoch 7/10\n",
            "16/16 [==============================] - 2s 115ms/sample - loss: 0.4937 - acc: 0.8125\n",
            "Epoch 8/10\n",
            "16/16 [==============================] - 2s 119ms/sample - loss: 0.4614 - acc: 0.8125\n",
            "Epoch 9/10\n",
            "16/16 [==============================] - 2s 138ms/sample - loss: 0.5298 - acc: 0.7500\n",
            "Epoch 10/10\n",
            "16/16 [==============================] - 2s 143ms/sample - loss: 0.4100 - acc: 0.9375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1f8adf9f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r22y8wDACxhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_sentences = [\n",
        "  \"That movie was absolutely awful\",\n",
        "  \"The acting was a bit lacking\",\n",
        "  \"The film was creative and surprising\",\n",
        "  \"Absolutely fantastic!\"\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GThbNdbJ3uwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getPredictionFromSentence(in_sentences):\n",
        "  labels = [\"Negative\", \"Positive\"]\n",
        "  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
        "  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "  test_input_ids = []\n",
        "  test_input_mask = []\n",
        "  test_segment_ids = []\n",
        "  test_label_ids = []\n",
        "  for feature in input_features:\n",
        "      test_input_ids.append(feature.input_ids)\n",
        "      test_input_mask.append(feature.input_mask)\n",
        "      test_segment_ids.append(feature.segment_ids)\n",
        "      test_label_ids.append(feature.label_id)\n",
        "  \n",
        "  probabilities = model.predict([test_input_ids, test_input_mask, test_segment_ids])\n",
        "  # print(probabilities)\n",
        "  predictions = (probabilities > 0.7).astype(np.int)\n",
        "  # print(predictions)\n",
        "  \n",
        "  return [(sentence, proba, labels[prediction[0]]) for sentence, proba, prediction in zip(in_sentences, probabilities, predictions)]\n",
        "\n",
        "def getPredictionFromFeatures(test_input_ids, test_input_mask, test_segment_ids):\n",
        "  model.predict([test_input_ids, test_input_mask, test_segment_ids])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSOAiujIDweR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "da3387ed-1878-45d2-e004-10d3400df1a0"
      },
      "source": [
        "predictions = getPredictionFromSentence(pred_sentences)\n",
        "print(predictions)\n",
        "predictions = getPredictionFromSentence(test_df['sentence'].tolist())\n",
        "print(predictions)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('That movie was absolutely awful', array([0.42205673], dtype=float32), 'Negative'), ('The acting was a bit lacking', array([0.4370236], dtype=float32), 'Negative'), ('The film was creative and surprising', array([0.457575], dtype=float32), 'Negative'), ('Absolutely fantastic!', array([0.44078428], dtype=float32), 'Negative')]\n",
            "[('he was in a merry mood', array([0.3441839], dtype=float32), 'Negative'), ('he was in gloomy mood', array([0.36388963], dtype=float32), 'Negative')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCM_2QijDyyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}